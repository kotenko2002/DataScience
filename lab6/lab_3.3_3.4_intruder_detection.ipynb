{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxNOG2qiT7bb"
   },
   "source": [
    "### <Center> Лабораторна робота №6. <br> Ідентифікація користувача за допомогою логістичної регресії"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 1572,
     "status": "ok",
     "timestamp": 1600568419975,
     "user": {
      "displayName": "Москаленко В'ячеслав Васильович",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN6Jzqki0z69x3j5PLOje08fXCxR7L_CW7QtyNXg=s64",
      "userId": "09799116143807457878"
     },
     "user_tz": -180
    },
    "id": "1_rN0IogT7be",
    "outputId": "f3fd4ebb-99b0-4f78-aecd-4239727aa1c2"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT_YKiR6T7br"
   },
   "source": [
    "### 1. Завантаження і перетворення даних\n",
    "Дані можна самостійно завантажити за посиланням [сторінка](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2). Однак можна цього не робити, оскільки дані вже завантажені для проведення лабораторної роботи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 4568,
     "status": "ok",
     "timestamp": 1600568515464,
     "user": {
      "displayName": "Москаленко В'ячеслав Васильович",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN6Jzqki0z69x3j5PLOje08fXCxR7L_CW7QtyNXg=s64",
      "userId": "09799116143807457878"
     },
     "user_tz": -180
    },
    "id": "sK2cb4cWT7bw",
    "outputId": "5a30c38e-df4f-435f-e5e8-e8178cb956d8"
   },
   "outputs": [],
   "source": [
    "# завантажимо навчальну і тестову вибірки\n",
    "train_df = pd.read_csv('lab6/data/train_sessions.csv',  index_col='session_id')\n",
    "test_df = pd.read_csv('lab6/data/test_sessions.csv', index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...               time6  site7  \\\n",
       "session_id                      ...                              \n",
       "21669                      NaT  ...                 NaT    NaN   \n",
       "54843                      NaT  ...                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ... 2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ... 2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ... 2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# приведемо колонку time1, ..., time10 до часового формату\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# відсортуємо дані за часом\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# подивимося на заголовок навчальної вибірки\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ycURxB3T7b5"
   },
   "source": [
    "В навчальній вибірці містяться наступні ознаки:\n",
    "    - site1 – індекс першого відвідування сайту в сесії\n",
    "    - time1 – час відвідування першого сайту в сесії\n",
    "    - ...\n",
    "    - site10 – індекс 10-го відвідування сайту в сесії\n",
    "    - time10 – час відвідування 10-го сайту в сесії\n",
    "    - target – цільова змінна, 1 для сесій Еліс, 0 для сесій інших користувачів\n",
    "    \n",
    "Сесії користувачів виділені таким чимном, щоб вони не можут бути довші півгодини чи 10 сайтів. Тобто сесія вважається закінченою або коли користувач відвідав 10 сайтів підряд або коли сесія зайняла за часом більше 30 хвилин.\n",
    "\n",
    "В таблиці зустрічаються пропущені значення, це значить, що сесія містить менше, ніж 10 сайтів. Замінимо пропущені значення нулями і приведемо ознаки до цільового типу. Також заванатажимо словник сайтів і подивимося, як він виглядає:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1600568520266,
     "user": {
      "displayName": "Москаленко В'ячеслав Васильович",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN6Jzqki0z69x3j5PLOje08fXCxR7L_CW7QtyNXg=s64",
      "userId": "09799116143807457878"
     },
     "user_tz": -180
    },
    "id": "Q82qF45IT7b7",
    "outputId": "89b3aeb2-3590-434b-d82b-2bf4f5b91150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всього сайтів: 48371\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25075</th>\n",
       "      <td>www.abmecatronique.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>groups.live.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42436</th>\n",
       "      <td>majeureliguefootball.wordpress.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>cdt46.media.tourinsoft.eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>www.hdwallpapers.eu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     site\n",
       "25075              www.abmecatronique.com\n",
       "13997                     groups.live.com\n",
       "42436  majeureliguefootball.wordpress.com\n",
       "30911           cdt46.media.tourinsoft.eu\n",
       "8104                  www.hdwallpapers.eu"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# приведемо колонки site1, ..., site10 до цілочислового формату і замінимо пропуски нулями\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype('int')\n",
    "test_df[sites] = test_df[sites].fillna(0).astype('int')\n",
    "\n",
    "# завантажимо словник сайтів\n",
    "with open(r\"lab6/data/site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# датафрейм словника сайтів\n",
    "sites_dict_df = pd.DataFrame(list(site_dict.keys()), \n",
    "                          index=list(site_dict.values()), \n",
    "                          columns=['site'])\n",
    "print(u'всього сайтів:', sites_dict_df.shape[0])\n",
    "sites_dict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZobaYZuT7cC"
   },
   "source": [
    "Виділимо цільову змінну і об'єднаємо вибірки, щоб разом привести їх до розрідженого формату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "1b6NQLhlT7cE"
   },
   "outputs": [],
   "source": [
    "# наша цільова змінна\n",
    "y_train = train_df['target']\n",
    "\n",
    "# об'єднана таблиця вхідних даних\n",
    "full_df = pd.concat([train_df.drop('target', axis=1), test_df])\n",
    "\n",
    "# індекс, за яким будемо відокремлювати навчальну вибірку від тестової\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcuO-EB0T7cN"
   },
   "source": [
    "Для самої першої моделі ми використовуємо лише відвідувані сайти в сесіях (але не будемо звертати увагу на часові ознаки). В основі такого вибору даних для моделей лежить така ідея: * у Еліс є свої улюблені сайти, і якщо ви ще побачите ці сайти в сесіях, тим вище ймовірність, що це сесія Еліс і навпаки. *\n",
    "\n",
    "Підготуємо дані, з усієї таблиці виберемо лише ознаки `site1, site2, ..., site10`. Нагадуємо, що пропущені значення замінені нулем. Ось як виглядатимуть перші рядки таблиць:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1600568525949,
     "user": {
      "displayName": "Москаленко В'ячеслав Васильович",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN6Jzqki0z69x3j5PLOje08fXCxR7L_CW7QtyNXg=s64",
      "userId": "09799116143807457878"
     },
     "user_tz": -180
    },
    "id": "Ah1VGxPDT7cP",
    "outputId": "01cb599a-27ec-462f-a677-d58532b07121"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>784</td>\n",
       "      <td>949</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>949</td>\n",
       "      <td>948</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>948</td>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>952</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "21669          56     55      0      0      0      0      0      0      0   \n",
       "54843          56     55     56     55      0      0      0      0      0   \n",
       "77292         946    946    951    946    946    945    948    784    949   \n",
       "114021        945    948    949    948    945    946    947    945    946   \n",
       "146670        947    950    948    947    950    952    946    951    946   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "21669            0  \n",
       "54843            0  \n",
       "77292          946  \n",
       "114021         946  \n",
       "146670         947  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# таблиця з індексами відвіданих сайтів в сесії\n",
    "full_sites = full_df[sites]\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIocdQ-BT7cb"
   },
   "source": [
    "Сесії представляють собою послідовність індексів сайтів і дані в такому вигляді невдалі для лінійних методів. Відповідно до нашої гіпотези (у Еліс є улюблені сайти) необхідно перетворити цю таблицю таким чином, щоб кожен можливий веб-сайт відповідав своєму окремому призначенню (колонка), а його значення зростало за кількістю відвідувачів цього веб-сайту в сесіях. Це робиться в два рядки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "VI3OsUzST7cc"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "3Ll_cv4XT7ci"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix#?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "soTapyJpT7co"
   },
   "outputs": [],
   "source": [
    "# послідовність з індексами\n",
    "sites_flatten = full_sites.values.flatten()\n",
    "\n",
    "# шкана матриця\n",
    "full_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n",
    "                                sites_flatten,\n",
    "                                range(0, sites_flatten.shape[0] + 10, 10)))[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUMiXftWT7cu"
   },
   "source": [
    "Ще один плюс використання розріджених матриць у тому, що для них є спеціальні реалізації як матричних операцій, так і алгоритми машинного навчання, що дозволяє істотно прискорити операції за рахунок особливостей структур даних. Це стосується і логістичної регресії. Ось тепер у нас все готове для побудови наших перших моделей.\n",
    "\n",
    "### 2. Побудова першої моделі\n",
    "\n",
    "Отже, у нас є алгоритм та дані для нього, побудуйте нашу першу модель, використовуючи реалізацію [логістичної регресії] (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) з пакета ` sklearn` з параметрами за замовчуванням. Перші 90% даних будемо використовувати для навчання (навчальна вибірка, відсортована за часом), а також 10% для перевірки якості (validation).\n",
    "\n",
    "**Напишіть просту функцію, яка поверне якість моделей на вкладеній вибірці, і навчіть наш перший класифікатор**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "DS4Y6Zg4T7cv"
   },
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, ratio = 0.9, seed=17):\n",
    "    '''\n",
    "    X, y – вибірка\n",
    "    ratio – у якому співвідношенні поділити вибірку\n",
    "    C, seed – коефіцієт регуляризації і random_state \n",
    "              логістичної регресії\n",
    "    '''\n",
    "    \n",
    "    # Розділяємо дані на навчальну та валідаційну вибірки\n",
    "    split_idx = int(X.shape[0] * ratio)\n",
    "    X_train, X_valid = X[:split_idx, :], X[split_idx:, :]\n",
    "    y_train, y_valid = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Навчаємо модель логістичної регресії\n",
    "    lr = LogisticRegression(C=C, random_state=seed, solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Робимо прогноз на валідаційній вибірці\n",
    "    y_pred_proba = lr.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    return roc_auc_score(y_valid, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drho7H3mT7c1"
   },
   "source": [
    "**Подивіться, який отримано ROC AUC на відкладеній вибірці.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "aZgYq5eGT7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на відкладеній вибірці: 0.9195\n"
     ]
    }
   ],
   "source": [
    "# Навчаємо перший класифікатор на розріджених даних сайтів\n",
    "X_train_sparse = full_sites_sparse[:idx_split]\n",
    "\n",
    "# Отримуємо ROC AUC на відкладеній вибірці\n",
    "auc_score = get_auc_lr_valid(X_train_sparse, y_train)\n",
    "print(f\"ROC AUC на відкладеній вибірці: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOcbcJt7T7c7"
   },
   "source": [
    "Будем вважати цю модель нашою першою відправною точкою (базовий рівень). Для побудови моделей для прогнозування на тестовій вибірці ** необхідно навчити модель заново вже на всій навчальній вибірці ** (покищо наша модель навчилася лише на частині даних), що підвищує її узагальнюючу здатність:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "caSGwpJNT7c8"
   },
   "outputs": [],
   "source": [
    "# функція для запису прогнозів в файлі\n",
    "def write_to_submission_file(predicted_labels, out_file, target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnrpDV3UT7dE"
   },
   "source": [
    "**Навчіть модель на всій вибірці, зробіть прогноз для тестової вибірки і покажіть результат**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO3SFJ87T7dF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.20990851e-03 4.81035990e-09 1.87260622e-08 2.35490144e-08\n",
      " 3.13025197e-05 2.18462443e-04 5.47935406e-04 1.32273071e-04\n",
      " 7.95167494e-04 1.03138077e-01]\n"
     ]
    }
   ],
   "source": [
    "X_train_full = full_sites_sparse[:idx_split]\n",
    "X_test_full = full_sites_sparse[idx_split:]\n",
    "\n",
    "# Створюємо та навчаємо модель логістичної регресії\n",
    "logit = LogisticRegression(C=1.0, random_state=17, solver='liblinear')\n",
    "logit.fit(full_sites_sparse[:idx_split], y_train)\n",
    "\n",
    "# Робимо прогноз на тестовій вибірці\n",
    "y_test_pred = logit.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_test_pred, 'lab6/result/submission.csv')\n",
    "\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4rdKvt6T7dL"
   },
   "source": [
    "### 3. Покращення моделі, побудова нових ознак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWaNbxfnT7dM"
   },
   "source": [
    "Створіть таку ознаку, яка буде представлят собою число формату ГГГГММ від тої дати, коли відбувалась сесія, наприклад 201407 -- 2014 рік і 7 месяц. Таким чином, ми будемо враховувати помісячний [линейный тренд](http://people.duke.edu/~rnau/411trend.htm) за весь період наданих даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "2zT4tSJ0T7dN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         time1  start_month\n",
      "session_id                                 \n",
      "21669      2013-01-12 08:05:57       201301\n",
      "54843      2013-01-12 08:37:23       201301\n",
      "77292      2013-01-12 08:50:13       201301\n",
      "114021     2013-01-12 08:50:17       201301\n",
      "146670     2013-01-12 08:50:20       201301\n",
      "242171     2013-01-12 08:50:22       201301\n",
      "57157      2013-01-12 08:50:25       201301\n",
      "240201     2013-01-12 08:50:28       201301\n",
      "210686     2013-01-12 08:50:31       201301\n",
      "98804      2013-01-12 08:50:37       201301\n"
     ]
    }
   ],
   "source": [
    "full_df['start_month'] = full_df['time1'].dt.year * 100 + full_df['time1'].dt.month\n",
    "print(full_df[['time1', 'start_month']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXjVHoXTT7dT"
   },
   "source": [
    "Додайте нову ознаку, попередньо нормалізуючи її за допомогою `StandardScaler`, і знову підрахуйте ROC AUC на відкладеній вибірці."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "jQ1Fl8aNT7dU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC з додаванням нормалізованої ознаки start_month: 0.9197\n",
      "ROC AUC тільки з сайтами: 0.9195\n",
      "Покращення: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Нормалізуємо ознаку start_month за допомогою StandardScaler\n",
    "scaler = StandardScaler()\n",
    "start_month_scaled = scaler.fit_transform(full_df[['start_month']])\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Додаємо нормалізовану ознаку start_month до розрідженої матриці сайтів\n",
    "full_sites_with_month = hstack([full_sites_sparse, start_month_scaled])\n",
    "\n",
    "# Перетворюємо в csr_matrix для підтримки індексації\n",
    "full_sites_with_month = csr_matrix(full_sites_with_month)\n",
    "\n",
    "# Розділяємо дані на навчальну та тестову частини\n",
    "X_train_with_month = full_sites_with_month[:idx_split]\n",
    "X_test_with_month = full_sites_with_month[idx_split:]\n",
    "\n",
    "# Підраховуємо ROC AUC на відкладеній вибірці з новою ознакою\n",
    "auc_score_with_month = get_auc_lr_valid(X_train_with_month, y_train)\n",
    "\n",
    "print(f\"ROC AUC з додаванням нормалізованої ознаки start_month: {auc_score_with_month:.4f}\")\n",
    "\n",
    "auc_score_sites_only = get_auc_lr_valid(full_sites_sparse[:idx_split], y_train)\n",
    "print(f\"ROC AUC тільки з сайтами: {auc_score_sites_only:.4f}\")\n",
    "print(f\"Покращення: {auc_score_with_month - auc_score_sites_only:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx1-wXJ8T7dY"
   },
   "source": [
    "**Додайте дві нові ознаки: start_hour і morning.**\n",
    "\n",
    "Ознака `start_hour` - це час у якому почалася сесія (від 0 до 23), а бінарна оознака` morning` рівна 1, якщо сесія почалася вранці і 0, якщо сесія почалася пізніше (будемо вважати, що це ранок, якщо `start_hour рівний` 11 або менше).\n",
    "\n",
    "**Підрахуйте RUC AUC на відкладеній вибірці для вибірки з:**\n",
    "- сайтами, `start_month` і` start_hour`\n",
    "- сайтами, `start_month` і` morning`\n",
    "- сайтами, `start_month`,` start_hour` і `morning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "RIwOQZ8_T7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC AUC (сайти + start_month + start_hour): 0.9579\n",
      "ROC AUC (сайти + start_month + morning): 0.9488\n",
      "ROC AUC (сайти + start_month + start_hour + morning): 0.9592\n"
     ]
    }
   ],
   "source": [
    "# Додаємо ознаку start_hour (час початку сесії від 0 до 23)\n",
    "full_df['start_hour'] = full_df['time1'].dt.hour\n",
    "\n",
    "# Додаємо бінарну ознаку morning (1 якщо start_hour <= 11, інакше 0)\n",
    "full_df['morning'] = (full_df['start_hour'] <= 11).astype(int)\n",
    "\n",
    "# Нормалізуємо start_hour\n",
    "scaler_hour = StandardScaler()\n",
    "start_hour_scaled = scaler_hour.fit_transform(full_df[['start_hour']])\n",
    "\n",
    "# Нормалізуємо morning (хоча це бінарна ознака, нормалізація не змінить результат)\n",
    "scaler_morning = StandardScaler()\n",
    "morning_scaled = scaler_morning.fit_transform(full_df[['morning']])\n",
    "\n",
    "# 1. Сайти + start_month + start_hour\n",
    "full_sites_month_hour = hstack([full_sites_sparse, start_month_scaled, start_hour_scaled])\n",
    "full_sites_month_hour = csr_matrix(full_sites_month_hour)\n",
    "X_train_month_hour = full_sites_month_hour[:idx_split]\n",
    "\n",
    "auc_sites_month_hour = get_auc_lr_valid(X_train_month_hour, y_train)\n",
    "print(f\"\\nROC AUC (сайти + start_month + start_hour): {auc_sites_month_hour:.4f}\")\n",
    "\n",
    "# 2. Сайти + start_month + morning\n",
    "full_sites_month_morning = hstack([full_sites_sparse, start_month_scaled, morning_scaled])\n",
    "full_sites_month_morning = csr_matrix(full_sites_month_morning)\n",
    "X_train_month_morning = full_sites_month_morning[:idx_split]\n",
    "\n",
    "auc_sites_month_morning = get_auc_lr_valid(X_train_month_morning, y_train)\n",
    "print(f\"ROC AUC (сайти + start_month + morning): {auc_sites_month_morning:.4f}\")\n",
    "\n",
    "# 3. Сайти + start_month + start_hour + morning\n",
    "full_sites_month_hour_morning = hstack([full_sites_sparse, start_month_scaled, start_hour_scaled, morning_scaled])\n",
    "full_sites_month_hour_morning = csr_matrix(full_sites_month_hour_morning)\n",
    "X_train_month_hour_morning = full_sites_month_hour_morning[:idx_split]\n",
    "\n",
    "auc_sites_month_hour_morning = get_auc_lr_valid(X_train_month_hour_morning, y_train)\n",
    "print(f\"ROC AUC (сайти + start_month + start_hour + morning): {auc_sites_month_hour_morning:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BICIGCSsT7df"
   },
   "source": [
    "### 4. Підбір коефіцієнта регуляризації\n",
    "\n",
    "Отже, ми ввели ознаки, які покращують якість нашої моделі у порівнянні з першим бейслайном. Чи можемо ми домогтися більшого значення метрики? Після того, як ми сформували навчальну та тестову вибірки, майже завжди має сенс підібрати оптимальні гіперпараметри - характеристики моделі, які не змінюються під час навчання. Наприклад, ви вивчали вирішальні дерева, глибина дерева це гіперпараметр, а ознака, за якому відбувається розгалуження і її значення - ні. У використовуваної нами логістичної регресії ваги кожної ознаки змінюються і під час навчання знаходяться їх оптимальні значення, а коефіцієнт регуляризації залишається постійним. Це той гіперпараметр, який ми зараз будемо оптимізувати.\n",
    "\n",
    "Порахуйте якість на відкладеній вибірці з коефіцієнтом регуляризації, який за замовчуванням `C = 1 ':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSd56_TtT7df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC з C=1.0: 0.9592\n"
     ]
    }
   ],
   "source": [
    "# Використовуємо найкращу комбінацію ознак: сайти + start_month + start_hour + morning\n",
    "auc_c1 = get_auc_lr_valid(X_train_month_hour_morning, y_train, C=1.0)\n",
    "print(f\"ROC AUC з C=1.0: {auc_c1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgyR4THVT7dm"
   },
   "source": [
    "Постараємося побити цей результат за рахунок оптимізації коефіцієнта регуляризації. Візьмемо набір можливих значень C і для кожного з них порахуємо значення метрики на відкладеної вибірці.\n",
    "\n",
    "Знайдіть `C` з` np.logspace (-3, 1, 10) `, при якому ROC AUC на відкладеної вибірці максимальний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "NUex1oHvT7dm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перевіряємо значення C: [1.00000000e-03 2.78255940e-03 7.74263683e-03 2.15443469e-02\n",
      " 5.99484250e-02 1.66810054e-01 4.64158883e-01 1.29154967e+00\n",
      " 3.59381366e+00 1.00000000e+01]\n",
      "C = 0.0010, ROC AUC = 0.8230\n",
      "C = 0.0028, ROC AUC = 0.8965\n",
      "C = 0.0077, ROC AUC = 0.9390\n",
      "C = 0.0215, ROC AUC = 0.9564\n",
      "C = 0.0599, ROC AUC = 0.9607\n",
      "C = 0.1668, ROC AUC = 0.9612\n",
      "C = 0.4642, ROC AUC = 0.9603\n",
      "C = 1.2915, ROC AUC = 0.9587\n",
      "C = 3.5938, ROC AUC = 0.9558\n",
      "C = 10.0000, ROC AUC = 0.9513\n",
      "\n",
      "Найкращий коефіцієнт регуляризації: C = 0.1668\n",
      "Найкращий ROC AUC: 0.9612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Набір можливих значень C\n",
    "C_values = np.logspace(-3, 1, 10)\n",
    "print(\"Перевіряємо значення C:\", C_values)\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "# Перебираємо всі значення C\n",
    "for C in C_values:\n",
    "    auc = get_auc_lr_valid(X_train_month_hour_morning, y_train, C=C)\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"C = {C:.4f}, ROC AUC = {auc:.4f}\")\n",
    "\n",
    "# Знаходимо найкращий C\n",
    "best_idx = np.argmax(auc_scores)\n",
    "best_C = C_values[best_idx]\n",
    "best_auc = auc_scores[best_idx]\n",
    "\n",
    "print(f\"\\nНайкращий коефіцієнт регуляризації: C = {best_C:.4f}\")\n",
    "print(f\"Найкращий ROC AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA64y9S-T7ds"
   },
   "source": [
    "Нарешті, навчіть модель зі знайденим оптимальним значенням коефіцієнта регуляризації і з побудованими ознаками `start_hour`,` start_month` і `morning`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHKSaYGpT7dt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.28566067e-04 2.85339939e-06 9.30993396e-07 6.40061712e-07\n",
      " 1.98825368e-04 7.07912966e-05 2.94004424e-03 1.85378224e-03\n",
      " 1.33199241e-04 3.15636324e-03]\n"
     ]
    }
   ],
   "source": [
    "X_test_final = full_sites_month_hour_morning[idx_split:]\n",
    "\n",
    "# Навчання моделі з оптимальним C на всій навчальній вибірці\n",
    "logit_optimized = LogisticRegression(C=best_C, random_state=17, solver='liblinear')\n",
    "logit_optimized.fit(X_train_month_hour_morning, y_train)\n",
    "\n",
    "# Прогноз на тестовій вибірці\n",
    "y_test_pred_optimized = logit_optimized.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_test_pred_optimized, 'lab6/result/submission_optimized.csv')\n",
    "\n",
    "print(y_test_pred_optimized[:10])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
